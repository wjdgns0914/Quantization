20180222
1)가장 간단한 256-10 MLP를 만들어서 MNIST를 돌려보니 잘된다, 그래서 cifar10을 돌렸다.
affine레이어에 들어가는 인풋이 [28,28,depth] 이런 형식이 아니라 [batch_size,28,28,depth]였다.
그래서 첫 dim만 빼고 나머지 dim을 서로 곱해서 쭉 펴주는 reshape를 진행하는 기존의 코드로 cifar10까지 그대로 진행이 가능했다.

2)cifar10을 돌려보니 잘안된다, 0.001로 돌리면 처음부터 10프로 나오고 ,0.0001의 LR로 돌리면 처음에 20프로 반짝 나오고 금방 10프로대로 추락하여 올라오지 못한다. MLP네트워크 크기를 더욱 키워야겠다.

다른 논문에서는 비트 수를 정해놓고 그걸로 양자화를 했으니 step size만 정하면 사실 양자화의 유효범위가 나왔다. 예를 들어 내가 레벨 5개를 쓸거고, step size를 2로 했으면 각 레벨은 -4,-2,0,2,4이기 때문에 이 양자화가 합리적으로 커버하는 유효범위는 [-6,6]이다. 즉 비트 수가 정해졌으니 step size만 정해주면 된다.
하지만 이 논문에서 최적화 대상이 되는 파라미터는 step size와 유효범위이다, 즉 유효범위가 늘어났는데 그 이유는 비트 수가 안정해졌기 때문이다. 유효범위가 주어지고 step size가 있으면 비트 수가 정해진다. 즉 비트 수가 처음부터 정해진게 아니라 우리가 찾는 것이다!

위에는 저렇게 썼지만 사실 이 논문에서도 비트 수는 정해주는거다, 다만 하한선과 상한선을 정해주면 양자화로 인해 생기는 오류를 overflow오류와 quantization오류로 나눠서 생각 할 수 있게 된다, 즉 편리하다. 그리고 기존처럼 Vmin을 안정해주면 0은 무조건 하나의 level로 들어가는걸로 암묵적으로 생각해야 최적화가 가능하다. 즉 이 논문에서 제시한 방식은 오차분석에서 유리하고 직관적이며, 0을 level에서 제외시킬 수 있다.

바이너라이즈 함수를 양자화함수로 바꾼고, 바이너라이즈를 디폴트값으로 하자.
